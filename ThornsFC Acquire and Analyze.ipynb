{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acquire and Analyze\n",
    "\n",
    "Utilizing the dataset share Twitter accounts, I ran scraped the data a second time, this time looking for tweets that include the hashtag #thornsfc. Using these tweets, I will then place it to a dataframe to analyze their information in python (but also allow it to be looked at in SQL) and finally add them to their own .csv file. Using user location, I will build a word cloud that depicts the locations of where users are tweeting about #thornsfc. It is interesting to see if majority of users are tweeting from portland, OR or Oregon in general, as this is where the Thorns FC soccer team is located, or if users are coming from various places across the northwest region, or even if they are being tweeted about in rivals citys such as Orlando, Seattle, and Los Angeles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import tweepy\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "from wordcloud import WordCloud, STOPWORDS \n",
    "\n",
    "from My_API_Keys import api_key, api_key_secret, access_token, access_token_secret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate the Tweepy API\n",
    "auth = tweepy.OAuthHandler(api_key,api_key_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth,wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_since = \"2019-01-01\"\n",
    "search_words = \"#ThornsFC\" + '-filter:retweets'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#created a variable to store the api search rather for future calls to pull data below and change up the output.\n",
    "\n",
    "tweets = tweepy.Cursor(api.search,\n",
    "               # tweet_mode is defaulted to short, which only holds the first 140 characters of a Tweet.\n",
    "               tweet_mode='extended',\n",
    "               q=search_words,\n",
    "               lang='en', since = date_since).items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list comprehension to pull date, user, location, and tweet\n",
    "\n",
    "user_info = [[item.created_at,item.user.screen_name, item.user.location, item.full_text] for item in tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#placed the user info into a dataframe for future analyzing within SQL or python\n",
    "\n",
    "tweet_text = pd.DataFrame(data=user_info, columns=['Date', 'User', 'Location', 'Tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_text.to_csv('thorn_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_text['Location'].value_counts()[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read file  \n",
    "df = pd.read_csv(r'thorn_tweets.csv', encoding =\"latin-1\") \n",
    "  \n",
    "comment_words = '' \n",
    "stopwords = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through the text file \n",
    "for val in df: \n",
    "      \n",
    "    # typecaste each val to string \n",
    "    val = str(val) \n",
    "  \n",
    "    # split the value \n",
    "    tokens = val.split() \n",
    "      \n",
    "    # Converts each token into lowercase \n",
    "    for i in range(len(tokens)): \n",
    "        tokens[i] = tokens[i].lower() \n",
    "      \n",
    "    comment_words += \" \".join(tokens)+\" \"\n",
    "  \n",
    "    wordcloud = WordCloud(width = 800, height = 800, \n",
    "                    background_color ='white', \n",
    "                    stopwords = stopwords, \n",
    "                    min_font_size = 10).generate(comment_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the WordCloud image                        \n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wordcloud) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grab Followers IDs - from twitter API assignment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm putting the handles in a list to iterate through below\n",
    "team_handles = ['ThornsFC', 'ORLPride']\n",
    "\n",
    "\n",
    "# This will iterate through each Twitter handle that we're collecting from\n",
    "for screen_name in team_handles:\n",
    "    \n",
    "    # Tells Tweepy we want information on the handle we're collecting from\n",
    "    # The next line specifies which information we want, which in this case is the number of followers \n",
    "    user = api.get_user(screen_name) \n",
    "    followers_count = user.followers_count\n",
    "\n",
    "    # Let's see roughly how long it will take to grab all the follower IDs. \n",
    "    print(f'''\n",
    "    @{screen_name} has {followers_count} followers. \n",
    "    That will take roughly {followers_count/(5000*60):.0f} hours and {followers_count/(5000):.2f} minutes\n",
    "    ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates a dictionary containing a list for each Twitter handle we'll be grabbing follower IDs from\n",
    "id_dict = {'ThornsFC' : [],\n",
    "           'ORLPride' : []}\n",
    "\n",
    "# Grabs the time when we start making requests to the API\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "# .keys() allows us to iterate through each key in the dictionary\n",
    "for handle in id_dict.keys():\n",
    "    \n",
    "    # Each page contains 5,000 records, so since we know there are much more than 5,000 followers for both\n",
    "    # the Thorns and Pride, we must iterate through each of the pages in order to get all follower IDs\n",
    "    # To grab the follower IDs, we will be using followers_ids\n",
    "    for page in tweepy.Cursor(api.followers_ids,\n",
    "                              # This is how we will get around the issue of not being able to grab all ids at once\n",
    "                              # Once the rate limit is hit, we will be notified that we must wait 15 mins (900 secs)\n",
    "                              wait_on_rate_limit=True, wait_on_rate_limit_notify=True, compression=True,\n",
    "                              screen_name=handle).pages():\n",
    "\n",
    "        # The page variable comes back as a list, so we have to use .extend rather than .append\n",
    "        id_dict[handle].extend(page)\n",
    "        \n",
    "\n",
    "# Let's see how long it took to grab all follower IDs\n",
    "end_time = datetime.datetime.now()\n",
    "elapsed_time = end_time - start_time\n",
    "print(elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_dict['ThornsFC'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = id_dict['ORLPride'][:10]\n",
    "\n",
    "for name in users:\n",
    "    \n",
    "    user = api.get_user(name)\n",
    "    print(user.screen_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grab descriptions based on the followers IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = ['screen_name', 'name', 'location', 'followers_count', 'friends_count', 'description']\n",
    "\n",
    "for team in id_dict.keys():\n",
    "    \n",
    "    # Descriptions with emoji or non-Roman letters can cause trouble. Encoding your .txt file in utf-8 will help\n",
    "    with open(f'{team}_followers.txt','w', encoding='utf-8') as out_file:\n",
    "        out_file.write('\\t'.join(headers) + '\\n')\n",
    "\n",
    "        for idx, ids in enumerate(id_dict[team]):\n",
    "            \n",
    "            # For accounts set to private, we won't be able to get the description unless we follow them\n",
    "            # Putting in a try/except statement, we can get around this issue.\n",
    "            try:\n",
    "                user = api.get_user(ids)\n",
    "                name = user.name\n",
    "                location = user.location\n",
    "                followers_count = user.followers_count\n",
    "                friends_count = user.friends_count\n",
    "                description = str(user.description).replace('\\t',' ').replace('\\n',' ')\n",
    "                outline = [user.screen_name, user.name, user.location, user.followers_count, \n",
    "                           user.friends_count, user.description]\n",
    "                \n",
    "                out_file.write('\\t'.join([str(item) for item in outline]) + '\\n')\n",
    "                \n",
    "            except:\n",
    "                continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
